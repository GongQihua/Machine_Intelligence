{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cw_attack.ipynb","provenance":[],"mount_file_id":"1K7VomU6_q8i_HK-LIqETrjqWjaTg7R_8","authorship_tag":"ABX9TyO5EzKuXmcTTrNmK5ztIPya"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["cd /content/drive/MyDrive/Colab Notebooks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZqUxufj4wehQ","executionInfo":{"status":"ok","timestamp":1651971543746,"user_tz":240,"elapsed":170,"user":{"displayName":"Qihua Gong","userId":"03267308857545272138"}},"outputId":"99c6d80b-bd0c-41ca-ba2b-1b5ec685446a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["from timeit import default_timer\n","class Timer(object):\n","    def __init__(self, msg='Starting.....', timer=default_timer, factor=1,\n","                 fmt=\"------- elapsed {:.4f}s --------\"):\n","        self.timer = timer\n","        self.factor = factor\n","        self.fmt = fmt\n","        self.end = None\n","        self.msg = msg\n","\n","    def __call__(self):\n","        return self.timer()\n","\n","    def __enter__(self):\n","        print(self.msg)\n","        self.start = self()\n","        return self\n","\n","    def __exit__(self, exc_type, exc_value, exc_traceback):\n","        self.end = self()\n","        print(str(self))\n","\n","    def __repr__(self):\n","        return self.fmt.format(self.elapsed)\n","\n","    def elapsed(self):\n","        if self.end is None:\n","            return (self() - self.start) * self.factor\n","        else:\n","            return (self.end - self.start) * self.factor"],"metadata":{"id":"klUwqMJW0vxM","executionInfo":{"status":"ok","timestamp":1652032806184,"user_tz":240,"elapsed":274,"user":{"displayName":"Qihua Gong","userId":"03267308857545272138"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGZBCCvzwW10","executionInfo":{"status":"ok","timestamp":1651971894654,"user_tz":240,"elapsed":349477,"user":{"displayName":"Qihua Gong","userId":"03267308857545272138"}},"outputId":"54422d5f-e8bc-488c-9a86-21d5c40179a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","\n","Loading MNIST\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:575: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/pooling.py:600: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:88: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:413: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs, training=training)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:98: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Initializing graph\n","\n","Training\n","\n","Train model\n","\n","Epoch 1/5\n","\n","Evaluating\n"," loss: 0.0645 acc: 0.9805\n","\n","Epoch 2/5\n"," batch 422/422\n","Evaluating\n"," loss: 0.0427 acc: 0.9867\n","\n","Epoch 3/5\n"," batch 422/422\n","Evaluating\n"," loss: 0.0445 acc: 0.9862\n","\n","Epoch 4/5\n","\n","Evaluating\n"," loss: 0.0424 acc: 0.9858\n","\n","Epoch 5/5\n"," batch 422/422\n","Evaluating\n"," loss: 0.0360 acc: 0.9890\n","\n"," Saving model\n","\n","Evaluating on clean data\n","\n","Evaluating\n"," loss: 0.0267 acc: 0.9909\n","\n","Generating adversarial data\n","\n","Making adversarials via CW\n","Batch 1/4   \n","------- elapsed 3.9777s --------\n","Batch 2/4   \n","------- elapsed 3.8179s --------\n","Batch 3/4   \n","------- elapsed 3.9130s --------\n","Batch 4/4   \n","------- elapsed 3.8011s --------\n","\n","Evaluating on adversarial data\n","\n","Evaluating\n"," loss: 0.8876 acc: 0.5938\n","\n","Predicting\n"," batch 1/1\n","\n","Predicting\n"," batch 1/1\n","\n","Saving figure\n"]}],"source":["import os\n","import numpy as np\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","from attacks import cw\n","\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","img_size = 28\n","img_chan = 1\n","n_classes = 10\n","batch_size = 32\n","\n","\n","print('\\nLoading MNIST')\n","\n","dataset = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = dataset.load_data()\n","X_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\n","X_train = X_train.astype(np.float32) / 255\n","X_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\n","X_test = X_test.astype(np.float32) / 255\n","\n","Categorical = tf.keras.utils.to_categorical\n","y_train = Categorical(y_train)\n","y_test = Categorical(y_test)\n","\n","ind = np.random.permutation(X_train.shape[0])\n","X_train, y_train = X_train[ind], y_train[ind]\n","\n","SPLIT = 0.1\n","n = int(X_train.shape[0] * (1-SPLIT))\n","X_valid = X_train[n:]\n","X_train = X_train[:n]\n","y_valid = y_train[n:]\n","y_train = y_train[:n]\n","\n","\n","\n","def model(x, logits=False, training=False):\n","    with tf.variable_scope('conv0'):\n","        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n","                             padding='same', activation=tf.nn.relu)\n","        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n","\n","    with tf.variable_scope('conv1'):\n","        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n","                             padding='same', activation=tf.nn.relu)\n","        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n","\n","    with tf.variable_scope('flatten'):\n","        shape = z.get_shape().as_list()\n","        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n","\n","    with tf.variable_scope('mlp'):\n","        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n","        z = tf.layers.dropout(z, rate=0.25, training=training)\n","\n","    LOGITS = tf.layers.dense(z, units=10, name='logits')\n","    y = tf.nn.softmax(LOGITS, name='ybar')\n","\n","    if logits:\n","        return y, LOGITS\n","    return y\n","\n","\n","class Dummy:\n","    pass\n","\n","\n","res = Dummy()\n","\n","\n","with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n","    res.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n","                           name='x')\n","    res.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n","    res.training = tf.placeholder_with_default(False, (), name='mode')\n","\n","    res.ybar, logits = model(res.x, logits=True, training=res.training)\n","\n","    with tf.variable_scope('acc'):\n","        count = tf.equal(tf.argmax(res.y, axis=1), tf.argmax(res.ybar, axis=1))\n","        res.acc = tf.reduce_mean(tf.cast(count, tf.float32), name='acc')\n","\n","    with tf.variable_scope('loss'):\n","        xent = tf.nn.softmax_cross_entropy_with_logits(labels=res.y,\n","                                                       logits=logits)\n","        res.loss = tf.reduce_mean(xent, name='loss')\n","\n","    with tf.variable_scope('train_op'):\n","        optimizer = tf.train.AdamOptimizer()\n","        vs = tf.global_variables()\n","        res.train_op = optimizer.minimize(res.loss, var_list=vs)\n","\n","    res.saver = tf.train.Saver()\n","\n","    res.x_fixed = tf.placeholder(\n","        tf.float32, (batch_size, img_size, img_size, img_chan),\n","        name='x_fixed')\n","    res.adv_eps = tf.placeholder(tf.float32, (), name='adv_eps')\n","    res.adv_y = tf.placeholder(tf.int32, (), name='adv_y')\n","\n","    optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n","    res.adv_train_op, res.xadv, res.noise = cw(model, res.x_fixed,\n","                                               y=res.adv_y, eps=res.adv_eps,\n","                                               optimizer=optimizer)\n","\n","print('\\nInitializing graph')\n","\n","res.sess = tf.InteractiveSession()\n","res.sess.run(tf.global_variables_initializer())\n","res.sess.run(tf.local_variables_initializer())\n","\n","\n","def evaluate(res, X_data, y_data, batch_size=128):\n","\n","    print('\\nEvaluating')\n","\n","    datasample = X_data.shape[0]\n","    databatch = int((datasample+batch_size-1) / batch_size)\n","    loss, acc = 0, 0\n","\n","    for batch in range(databatch):\n","        print(' batch {0}/{1}'.format(batch + 1, databatch), end='\\r')\n","        start = batch * batch_size\n","        end = min(datasample, start + batch_size)\n","        cnt = end - start\n","        batch_loss, batch_acc = res.sess.run(\n","            [res.loss, res.acc],\n","            feed_dict={res.x: X_data[start:end],\n","                       res.y: y_data[start:end]})\n","        loss += batch_loss * cnt\n","        acc += batch_acc * cnt\n","    loss /= datasample\n","    acc /= datasample\n","\n","    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n","    return loss, acc\n","\n","\n","def train(res, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n","          load=False, shuffle=True, batch_size=128, name='model'):\n","\n","    if load:\n","        if not hasattr(res, 'saver'):\n","            return print('\\nError: cannot find saver op')\n","        print('\\nLoading saved model')\n","        return res.saver.restore(res.sess, 'model/{}'.format(name))\n","\n","    print('\\nTrain model')\n","    datasample = X_data.shape[0]\n","    databatch = int((datasample+batch_size-1) / batch_size)\n","    for epoch in range(epochs):\n","        print('\\nEpoch {0}/{1}'.format(epoch + 1, epochs))\n","\n","        if shuffle:\n","            ind = np.arange(datasample)\n","            np.random.shuffle(ind)\n","            X_data = X_data[ind]\n","            y_data = y_data[ind]\n","\n","        for batch in range(databatch):\n","            print(' batch {0}/{1}'.format(batch + 1, databatch), end='\\r')\n","            start = batch * batch_size\n","            end = min(datasample, start + batch_size)\n","            res.sess.run(res.train_op, feed_dict={res.x: X_data[start:end],\n","                                                  res.y: y_data[start:end],\n","                                                  res.training: True})\n","        if X_valid is not None:\n","            evaluate(res, X_valid, y_valid)\n","\n","    if hasattr(res, 'saver'):\n","        print('\\n Saving model')\n","        os.makedirs('model', exist_ok=True)\n","        res.saver.save(res.sess, 'model/{}'.format(name))\n","\n","\n","def predict(%rehashx, X_data, batch_size=128):\n","\n","    print('\\nPredicting')\n","    n_classes = res.ybar.get_shape().as_list()[1]\n","\n","    datasample = X_data.shape[0]\n","    databatch = int((datasample+batch_size-1) / batch_size)\n","    yval = np.empty((datasample, n_classes))\n","\n","    for batch in range(databatch):\n","        print(' batch {0}/{1}'.format(batch + 1, databatch), end='\\r')\n","        start = batch * batch_size\n","        end = min(datasample, start + batch_size)\n","        y_batch = res.sess.run(res.ybar, feed_dict={res.x: X_data[start:end]})\n","        yval[start:end] = y_batch\n","    print()\n","    return yval\n","\n","\n","def make_cw(res, X_data, epochs=1, eps=0.1, batch_size=batch_size):\n","\n","    print('\\nMaking adversarials via CW')\n","\n","    datasample = X_data.shape[0]\n","    databatch = int((datasample + batch_size - 1) / batch_size)\n","    X_adv = np.empty_like(X_data)\n","\n","    for batch in range(databatch):\n","        with Timer('Batch {0}/{1}   '.format(batch + 1, databatch)):\n","            end = min(datasample, (batch+1) * batch_size)\n","            start = end - batch_size\n","            feed_dict = {\n","                res.x_fixed: X_data[start:end],\n","                res.adv_eps: eps,\n","                res.adv_y: np.random.choice(n_classes)}\n","            res.sess.run(res.noise.initializer)\n","            for epoch in range(epochs):\n","                res.sess.run(res.adv_train_op, feed_dict=feed_dict)\n","\n","            xadv = res.sess.run(res.xadv, feed_dict=feed_dict)\n","            X_adv[start:end] = xadv\n","\n","    return X_adv\n","\n","\n","print('\\nTraining')\n","\n","train(res, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n","      name='mnist')\n","\n","print('\\nEvaluating on clean data')\n","\n","evaluate(res, X_test, y_test)\n","\n","print('\\nGenerating adversarial data')\n","\n","datasample = 128\n","ind = np.random.choice(X_test.shape[0], size=datasample, replace=False)\n","X_test = X_test[ind]\n","y_test = y_test[ind]\n","\n","X_adv = make_cw(res, X_test, eps=0.002, epochs=100)\n","\n","print('\\nEvaluating on adversarial data')\n","\n","evaluate(res, X_adv, y_test)\n","\n","\n","y1 = predict(res, X_test)\n","y2 = predict(res, X_adv)\n","\n","z0 = np.argmax(y_test, axis=1)\n","z1 = np.argmax(y1, axis=1)\n","z2 = np.argmax(y2, axis=1)\n","\n","ind = np.logical_and(z0 == z1, z1 != z2)\n","\n","ind = z0 == z1\n","\n","X_test = X_test[ind]\n","X_adv = X_adv[ind]\n","z1 = z1[ind]\n","z2 = z2[ind]\n","y2 = y2[ind]\n","\n","ind, = np.where(z1 != z2)\n","cur = np.random.choice(ind, size=n_classes)\n","X_org = np.squeeze(X_test[cur])\n","X_tmp = np.squeeze(X_adv[cur])\n","y_tmp = y2[cur]\n","\n","fig = plt.figure(figsize=(n_classes+0.2, 3.2))\n","gs = gridspec.GridSpec(3, n_classes+1, width_ratios=[1]*n_classes + [0.1],\n","                       wspace=0.01, hspace=0.01)\n","\n","label = np.argmax(y_tmp, axis=1)\n","proba = np.max(y_tmp, axis=1)\n","\n","for i in range(n_classes):\n","    ans = fig.add_subplot(gs[0, i])\n","    ans.imshow(X_org[i], cmap='gray', interpolation='none')\n","    ans.set_xticks([])\n","    ans.set_yticks([])\n","\n","    ans = fig.add_subplot(gs[1, i])\n","    img = ans.imshow(X_tmp[i]-X_org[i], cmap='RdBu_r', vmin=-1,\n","                    vmax=1, interpolation='none')\n","    ans.set_xticks([])\n","    ans.set_yticks([])\n","\n","    ans = fig.add_subplot(gs[2, i])\n","    ans.imshow(X_tmp[i], cmap='gray', interpolation='none')\n","    ans.set_xticks([])\n","    ans.set_yticks([])\n","\n","    ans.set_xlabel('{0} ({1:.2f})'.format(label[i], proba[i]), fontsize=12)\n","\n","ans = fig.add_subplot(gs[1, n_classes])\n","dummy = plt.cm.ScalarMappable(cmap='RdBu_r', norm=plt.Normalize(vmin=-1,                                                                vmax=1))\n","dummy.set_array([])\n","fig.colorbar(mappable=dummy, cax=ans, ticks=[-1, 0, 1], ticklocation='right')\n","\n","print('\\nSaving figure')\n","\n","gs.tight_layout(fig)\n","os.makedirs('img', exist_ok=True)\n","plt.savefig('img/cw2_mnist.png')"]}]}